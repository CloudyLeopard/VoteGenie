{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "verified_data_file_path = \"../verified_data_01_15_2024.xlsx\"\n",
    "df_verified = pd.read_excel(verified_data_file_path)\n",
    "df_verified.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VoteGenie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genie_master import GenieMaster\n",
    "\n",
    "genie_db_path = \"./chroma_qadata_db/\"\n",
    "gm = GenieMaster(db_path=genie_db_path)\n",
    "print(\"Genie is ready...\", gm.model_is_ready())\n",
    "print(\"Document collection count...\", gm._document_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # make a list of genies for each unique politician\n",
    "# names = df_verified.name.unique()\n",
    "# genies = {n: gm.get_genie(n) for n in names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_row(row):\n",
    "#     output = genies[row[\"name\"]].ask(row[\"question\"])\n",
    "#     result = output[\"result\"]\n",
    "#     row['prediction'] = result.get('answer', \"\")\n",
    "#     row['reasoning'] = result.get('reasoning', \"\")\n",
    "#     row['evidence'] = result.get('evidence', \"\")\n",
    "#     row['cost'] = output['total_cost']\n",
    "\n",
    "#     print(\"NAME:\", row[\"name\"])\n",
    "#     print(\"QUESTION:\", row[\"question\"])\n",
    "#     print(\"ANSWER:\", row[\"stance\"])\n",
    "#     print(\"PREDICTION:\", row[\"prediction\"])\n",
    "#     print(\"-\"*30)\n",
    "\n",
    "\n",
    "# df_results = df_verified.apply(process_row, axis=1)\n",
    "# df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vote_easy_genie_lib import get_df_results\n",
    "output_csv = \"temp/temp.csv\"\n",
    "output_xlsx = \"verify_result.xlsx\"\n",
    "df_results = await get_df_results(df_verified, vectorstore_path=genie_db_path, output_csv=output_csv, output_xlsx=output_xlsx, model_name=\"gpt-4-1106-preview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_answer(df, prediction_col_name, verification_col_name=\"stance\"):\n",
    "    \"\"\"returns a list where if ans and pred is the same, True; if ans and pred is \n",
    "    different, False\"\"\"\n",
    "    df[prediction_col_name] = df[prediction_col_name].str.lower()\n",
    "    df[verification_col_name] = df[verification_col_name].str.lower()\n",
    "    return np.where((df[prediction_col_name] == df[verification_col_name]), True, False)\n",
    "prediction_col_name=\"answer\"\n",
    "df_incorrect_entries = df_results.loc[~check_answer(df_results, prediction_col_name=prediction_col_name)]\n",
    "print(f\"Accuracy: {df_results.shape[0]-df_incorrect_entries.shape[0]}/{df_results.shape[0]}\")\n",
    "print(f\"Percentage accuracy: {1-df_incorrect_entries.shape[0]/df_results.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_col_name=\"answer\"\n",
    "for _, row in df_incorrect_entries.iterrows():\n",
    "    print(\"NAME:\", row[\"name\"])\n",
    "    print(\"QUESTION:\", row[\"question\"])\n",
    "    print(\"Prediction\\tAnswer\")\n",
    "    print(f\"{row[prediction_col_name]}\\t\\t{row['stance']}\")\n",
    "    print(f\"Reasoning\\t{row['reasoning']}\")\n",
    "    print(f\"Evidence\\t{row['evidence']}\")\n",
    "    print(f\"Source\\t\\t{row['source_content']}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in df_results.loc[check_answer(df_results, prediction_col_name=prediction_col_name)].iterrows():\n",
    "    print(\"NAME:\", row[\"name\"])\n",
    "    print(\"QUESTION:\", row[\"question\"])\n",
    "    print(\"Prediction\\tAnswer\")\n",
    "    print(f\"{row[prediction_col_name]}\\t\\t{row['stance']}\")\n",
    "    print(f\"Reasoning\\t{row['reasoning']}\")\n",
    "    print(f\"Evidence\\t{row['evidence']}\")\n",
    "    print(f\"Source\\t\\t{row['source_content']}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAGAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel(\"verify_result.xlsx\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    context_precision\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "df_temp = df.loc[:3]\n",
    "result = evaluate(\n",
    "    df,\n",
    "    metrics=[\n",
    "       answer_relevancy,\n",
    "        faithfulness,\n",
    "        context_recall,\n",
    "        context_precision \n",
    "    ],\n",
    "    column_map={\n",
    "        \"\"\n",
    "    }\n",
    ")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel(\"verify_result.xlsx\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Joe Biden\"\n",
    "questions = df.question[:5].to_list()\n",
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genie_master import GenieMaster\n",
    "\n",
    "genie_db_path = \"./chroma_qadata_db/\"\n",
    "gm = GenieMaster(db_path=genie_db_path)\n",
    "print(\"Genie is ready...\", gm.model_is_ready())\n",
    "print(\"Document collection count...\", gm._document_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "genie = gm.get_genie(name)\n",
    "print(genie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "results_1 = list()\n",
    "for q in questions[:1]:\n",
    "    results_1.append(genie.ask(q))\n",
    "    print(\"Question answered\")\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "results_2 = genie.batch_ask(questions)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_2['results'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "results_3 = await genie.async_batch_ask(questions)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "base_batch_result = genie.base_batch_ask(questions)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAGAS evaluation\n",
    "from ragas.langchain.evalchain import RagasEvaluatorChain\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    ")\n",
    "from ragas import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_batch_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_samples = {\n",
    "    \"question\": questions,\n",
    "    \"contexts\": [[doc.page_content for doc in r[\"context\"]] for r in base_batch_result],\n",
    "    \"answer\": [json.dumps(r[\"result\"]) for r in base_batch_result],\n",
    "    \"ground_truths\": [[\"yes\"]*4]*5\n",
    "}\n",
    "dataset = Dataset.from_dict(data_samples)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(\n",
    "    dataset,\n",
    "    metrics=[\n",
    "        # context_precision,\n",
    "        faithfulness,\n",
    "        # answer_relevancy,\n",
    "        # context_recall,\n",
    "    ],\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sample_2 = base_batch_result.copy()\n",
    "data_sample_2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in data_sample_2:\n",
    "    entry[\"source_documents\"] = entry.pop(\"context\")\n",
    "    entry[\"query\"] = entry.pop(\"question\")\n",
    "    entry[\"result\"] = json.dumps(entry[\"result\"])\n",
    "data_sample_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval = {\n",
    "#     m.name: RagasEvaluatorChain(metric=m).evaluate(\n",
    "#         questions,\n",
    "#         data_sample_2\n",
    "#     )\n",
    "#     for m in [\n",
    "#         faithfulness,\n",
    "#         answer_relevancy,\n",
    "#         context_precision,\n",
    "#         context_recall,\n",
    "#     ]\n",
    "# }\n",
    "eval = RagasEvaluatorChain(metric=faithfulness)(data_sample_2[0])\n",
    "eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40d4d8f1a38d49e6add4bae7402d6536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/115k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "117caf58cb4e4007a7cc16c45481c989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating baseline split:   0%|          | 0/30 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "from datasets import load_dataset\n",
    "\n",
    "fiqa_eval = load_dataset(\"explodinggradients/fiqa\", \"ragas_eval\")\n",
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "\n",
    "from ragas import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_precision]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [faithfulness]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:40<00:00, 40.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_relevancy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_recall]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:08<00:00,  8.68s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'context_precision': 0.3333, 'faithfulness': 0.9444, 'answer_relevancy': 0.9756, 'context_recall': 0.9524}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = evaluate(\n",
    "    fiqa_eval[\"baseline\"].select(range(3)), # selecting only 3\n",
    "    metrics=[\n",
    "        context_precision,\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        context_recall,\n",
    "    ],\n",
    ")\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uslong",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
